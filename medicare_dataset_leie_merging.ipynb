{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1e392c",
   "metadata": {},
   "source": [
    "### This code allows to merge LEIE Dataset and Medicare Part D dataset with Selected Columns and Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44d9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33d402",
   "metadata": {},
   "source": [
    "# 1- Processing LEIE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06406d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the 'LEIE_UPDATED.csv' file into a pandas DataFrame \n",
    "leie = pd.read_csv('LEIE_UPDATED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the dimensions\n",
    "leie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ced7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names\n",
    "print(\"Column Names:\")\n",
    "print(leie.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "leie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50859047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of specific exclusion codes to be used for filtering the dataset (from paper)\n",
    "exclusion_codes = ['1128a1', '1128a2', '1128a3', '1128b4', '1128b7', '1128c3gi', '1128c3gii']\n",
    "\n",
    "# Filter the 'leie' DataFrame to include only rows where the 'EXCLTYPE' column's value is present in the 'exclusion_codes' list.\n",
    "filtered_leie = leie[leie.EXCLTYPE.isin(exclusion_codes)]\n",
    "filtered_leie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b92adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'EXCLDATE' column to datetime objects using the specified year-month-day format.\n",
    "filtered_leie['EXCLDATE'] = pd.to_datetime(filtered_leie.EXCLDATE, format='%Y%m%d') #most important value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de69595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exclusion_end(exclusion_date, end_year): \n",
    "    \"\"\"\n",
    "        # Defines a function to calculate the exclusion end year \n",
    "        # based on the exclusion date and a given end year.\n",
    "    \"\"\"\n",
    "    month = exclusion_date.month # Extracts the month from the 'exclusion_date'.\n",
    "    if month > 6: # Checks if the exclusion month is after June (i.e., July to December).\n",
    "        return end_year + 1 \n",
    "    else: # If the exclusion month is June or earlier (i.e., January to June).\n",
    "        return end_year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_leie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b1d22",
   "metadata": {},
   "source": [
    "# 2- Processing Medicare Part D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40324c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the 'Combined_Medicare_2017_2019.csv' file into a pandas DataFrame \n",
    "partd_main = pd.read_csv('outputs/Combined_Medicare_2017_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33488238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the dimensions\n",
    "partd_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c78fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column Names:\")\n",
    "print(partd_main.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceed936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renames the column 'Prscrbr_NPI' to 'npi' in the 'partd_main' DataFrame, modifying the DataFrame in place.\n",
    "partd_main.rename({'Prscrbr_NPI':'npi'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9edd56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NPI lowercase so we can merge on it later. \n",
    "filtered_leie.rename({'NPI':'npi'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48450998",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Renames the column 'Source_Year' to 'DATA_YEAR' in the 'partd_main' DataFrame, modifying the DataFrame in place.\n",
    "partd_main.rename(columns={'Source_Year': 'DATA_YEAR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces all occurrences of '0' with '5' in the 'Tot_Benes' column of the 'partd_main' DataFrame.\n",
    "partd_main.Tot_Benes = partd_main.Tot_Benes.replace(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 'partd_main' DataFrame by 'npi', 'Prscrbr_Type', and 'DATA_YEAR' columns.\n",
    "# For each group, compute multiple aggregation statistics (sum, mean, median, std, min, max)\n",
    "# on the following numeric columns: 'Tot_Benes', 'Tot_Clms', 'Tot_30day_Fills',\n",
    "# 'Tot_Day_Suply', and 'Tot_Drug_Cst'. Then print the resulting DataFrame's shape.\n",
    "\n",
    "\n",
    "partd_main = partd_main.groupby(['npi','Prscrbr_Type', 'DATA_YEAR']).agg({'Tot_Benes':['sum', 'mean', 'median', np.std, 'min', 'max'],\n",
    "                                    'Tot_Clms':['sum', 'mean', 'median', np.std, 'min', 'max'],\n",
    "                                    'Tot_30day_Fills':['sum', 'mean', 'median', np.std, 'min', 'max'],\n",
    "                                    'Tot_Day_Suply':['sum', 'mean', 'median', np.std, 'min', 'max'],\n",
    "                                    'Tot_Drug_Cst': ['sum', 'mean', 'median', np.std, 'min', 'max']})\n",
    "print(partd_main.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea398ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the MultiIndex columns resulting from the aggregation by joining each level with an underscore.\n",
    "# For example, ('Tot_Benes', 'sum') becomes 'Tot_Benes_sum'.\n",
    "\n",
    "partd_main.columns = ['_'.join(col) for col in partd_main.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddc84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd_main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for the specified grouping columns ('Prscrbr_Type', 'npi', 'DATA_YEAR'),\n",
    "# moving them back into regular columns in the DataFrame for easier access and analysis.\n",
    "\n",
    "partd_main = partd_main.reset_index(level=['Prscrbr_Type', 'npi', 'DATA_YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20473696",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f26804",
   "metadata": {},
   "source": [
    "# 3- Merging Medicare Part D and LEIE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdde8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 'filtered_leie' and 'partd_main' DataFrames on the 'npi' column using an outer join.\n",
    "# This ensures that all records from both DataFrames are retained, with NaNs where data is missing.\n",
    "\n",
    "\n",
    "partd = pd.merge(filtered_leie, partd_main, on='npi', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and subset the 'partd' DataFrame to include only the relevant columns:\n",
    "# - Identifier and grouping columns: 'npi', 'Prscrbr_Type', 'DATA_YEAR'\n",
    "# - Aggregated statistics for prescription-related metrics (e.g., beneficiaries, claims, cost)\n",
    "# - LEIE exclusion-related columns: 'EXCLTYPE', 'EXCLDATE', 'WVRSTATE'\n",
    "# This helps streamline the dataset for downstream analysis or modeling.\n",
    "\n",
    "\n",
    "partd = partd[[\n",
    "        'npi', 'Prscrbr_Type', 'DATA_YEAR',       \n",
    "       'Tot_Benes_sum', 'Tot_Benes_mean', 'Tot_Benes_median',\n",
    "       'Tot_Benes_std', 'Tot_Benes_min', 'Tot_Benes_max',\n",
    "\n",
    "       'Tot_Clms_sum', 'Tot_Clms_mean',\n",
    "       'Tot_Clms_median', 'Tot_Clms_std',\n",
    "       'Tot_Clms_min', 'Tot_Clms_max',\n",
    "\n",
    "       'Tot_30day_Fills_sum', 'Tot_30day_Fills_mean',\n",
    "       'Tot_30day_Fills_median', 'Tot_30day_Fills_std',\n",
    "       'Tot_30day_Fills_min', 'Tot_30day_Fills_max',\n",
    "\n",
    "       'Tot_Day_Suply_sum', 'Tot_Day_Suply_mean',\n",
    "       'Tot_Day_Suply_median', 'Tot_Day_Suply_std',\n",
    "       'Tot_Day_Suply_min', 'Tot_Day_Suply_max',\n",
    "       \n",
    "       'Tot_Drug_Cst_sum','Tot_Drug_Cst_mean', \n",
    "       'Tot_Drug_Cst_median', 'Tot_Drug_Cst_std',\n",
    "       'Tot_Drug_Cst_min', 'Tot_Drug_Cst_max', \n",
    "       \n",
    "       'EXCLTYPE',\n",
    "       'EXCLDATE', 'WVRSTATE' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24c154",
   "metadata": {},
   "source": [
    "### Remove rows without npi or provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45459962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'npi' is 0 (invalid identifier) or 'Prscrbr_Type' is missing/empty.\n",
    "# This ensures that only valid, non-null prescriber records are retained in the DataFrame.\n",
    "\n",
    "\n",
    "partd = partd[(partd.npi != 0) & (partd.Prscrbr_Type)]\n",
    "partd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79720672",
   "metadata": {},
   "source": [
    "### Add target (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'TARGET' to the 'partd' DataFrame and set its default value to '0' (as a string).\n",
    "# This will likely be used later for labeling or classification purposes (e.g., marking non-excluded providers).\n",
    "\n",
    "\n",
    "partd['TARGET'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the 'EXCLDATE' datetime column and store it in a new column 'START_EXCLDATE'.\n",
    "# This helps in analyzing or grouping exclusions by year.\n",
    "\n",
    "\n",
    "partd['START_EXCLDATE'] = partd['EXCLDATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b88970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(start_exc, data_yr):\n",
    "    \"\"\"\n",
    "    Assigns a fraud label based on the relationship between exclusion year and data year.\n",
    "\n",
    "    Parameters:\n",
    "    - start_exc (int or float): The year the provider was excluded (e.g., from EXCLDATE).\n",
    "    - data_yr (int): The year of the Part D dataset record.\n",
    "\n",
    "    Returns:\n",
    "    - str: 'FRAUD' if the exclusion happened after the data year (i.e., provider was still active),\n",
    "           'NOT_FRAUD' otherwise (i.e., already excluded or clean).\n",
    "    \"\"\"\n",
    "    if start_exc > data_yr:\n",
    "        return 'FRAUD'\n",
    "    else:\n",
    "        return 'NOT_FRAUD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'TARGET' column by applying the 'make_labels' function row-wise,\n",
    "# comparing 'START_EXCLDATE' and 'DATA_YEAR' to assign fraud labels for each record.\n",
    "\n",
    "partd['TARGET'] = partd[[\"START_EXCLDATE\",\"DATA_YEAR\"]].apply(lambda x: make_labels(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb310627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of unique label values present in the 'TARGET' column,\n",
    "# ordered by their frequency from most to least common.\n",
    "\n",
    "\n",
    "partd.TARGET.value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd.TARGET.value_counts()\n",
    "\n",
    "# TARGET\n",
    "# NOT_FRAUD    2854225\n",
    "# FRAUD           2614\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6face0e",
   "metadata": {},
   "source": [
    "# 4- Downsizing Dataset Randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total samples desired\n",
    "desired_total = 1_000_000\n",
    "\n",
    "# Calculate number of samples per class based on the original ratio\n",
    "fraud_ratio = partd[\"TARGET\"].value_counts(normalize=True)[\"FRAUD\"]\n",
    "not_fraud_ratio = partd[\"TARGET\"].value_counts(normalize=True)[\"NOT_FRAUD\"]\n",
    "\n",
    "num_fraud = int(desired_total * fraud_ratio)\n",
    "num_not_fraud = desired_total - num_fraud  # or int(desired_total * not_fraud_ratio)\n",
    "\n",
    "print(f\"Sampling {num_fraud} FRAUD and {num_not_fraud} NOT_FRAUD records.\")\n",
    "\n",
    "# Separate classes\n",
    "df_fraud = partd[partd[\"TARGET\"] == \"FRAUD\"]\n",
    "df_not_fraud = partd[partd[\"TARGET\"] == \"NOT_FRAUD\"]\n",
    "\n",
    "# Sample with replacement = False (downsampling)\n",
    "df_fraud_sampled = df_fraud.sample(n=num_fraud, random_state=42)\n",
    "df_not_fraud_sampled = df_not_fraud.sample(n=num_not_fraud, random_state=42)\n",
    "\n",
    "# Combine back\n",
    "df_sampled = pd.concat([df_fraud_sampled, df_not_fraud_sampled])\n",
    "\n",
    "# Shuffle the resulting dataframe (optional but recommended)\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check new class distribution\n",
    "print(\"New TARGET distribution:\")\n",
    "print(df_sampled[\"TARGET\"].value_counts())\n",
    "print(df_sampled[\"TARGET\"].value_counts(normalize=True))\n",
    "print(f\"New dataset size: {len(df_sampled)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdbd17",
   "metadata": {},
   "source": [
    "# 5- One Hot Encoding for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "333f0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "partd_category_columns = ['Prscrbr_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns listed in 'partd_category_columns' into one-hot encoded dummy variables,\n",
    "# dropping the first category in each to avoid multicollinearity in modeling.\n",
    "\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=partd_category_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59269bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns to drop from the dataset, typically because they are identifiers,\n",
    "# date-related, or exclusion info not needed for model training or further analysis.\n",
    "\n",
    "\n",
    "columns_to_drop = [ 'EXCLTYPE','EXCLDATE',\n",
    "                   'WVRSTATE',\n",
    "                   'START_EXCLDATE', 'npi', 'DATA_YEAR']\n",
    "df_sampled.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all missing (NaN) values in 'df_sampled' with 0 to avoid issues in modeling or analysis,\n",
    "# then display the first few rows to verify the changes.\n",
    "\n",
    "df_sampled.fillna(0, inplace=True)\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f98317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd19aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame 'df_sampled' to a CSV file without the index column,\n",
    "# creating a downsized dataset for future use or sharing.\n",
    "\n",
    "\n",
    "df_sampled.to_csv(\"Combined_LEIE_Medicare_2017_2019_DOWNSIZED_1mil.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_v_1_13_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
