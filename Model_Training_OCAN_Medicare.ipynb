{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b0387-feaa-45a3-834c-c77b6b28d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52483d20-5531-46bc-b3cf-8f4bb0e4deec",
   "metadata": {},
   "source": [
    "## 1- Load Part D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a5235-4cb5-4b43-a0c8-39e5156c753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read Medicare Part D data and LEIE data.\n",
    "combined_df = pd.read_csv(\"outputs/Combined_LEIE_Medicare_2017_2019_DOWNSIZED_1mil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45963431-63c6-4ddf-89b9-5b7a44cf694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581e285-6cb8-4b18-bff1-7c054bf8df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inspect column names to avoid KeyError. \n",
    "print(\"Combined columns:\", combined_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a355b-4cf8-4c9d-96fb-57e0ae929a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quick sanity check: \n",
    "print(\"\\n=== Medicare Merged Dataset ===\")\n",
    "print(\"Merged Dataset shape:\", combined_df.shape)\n",
    "print(\"FraudFlag distribution:\\n\", combined_df[\"TARGET\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f728d77-e6bd-4947-a03a-fab145fa24b8",
   "metadata": {},
   "source": [
    "## 2- Normalization and Mapping of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad23f4-004b-4b30-a41e-75c84cf38a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We want to select only truly numeric features (excluding identifiers).\n",
    "#    Using select_dtypes prevents KeyError that arises if you manually list wrong names.\n",
    "numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"\\nAll numeric columns (including IDs & target):\\n\", numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5e231-1f92-481e-bd9b-625e2ec90c67",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899b812-dcba-497d-8260-f9b6108ce9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. From these numeric columns, drop 'NPI' (identifier) and 'FraudFlag' (target).\n",
    "features_list = [col for col in numeric_cols if col not in [\"npi\", \"TARGET\"]]\n",
    "print(\"\\nFeatures selected for modeling (numeric_cols without 'NPI' and 'FraudFlag'):\\n\", features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9fe27-0f6d-46d6-adc9-5615401608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a StandardScaler instance to normalize these numeric features.\n",
    "scaler_med = StandardScaler()\n",
    "\n",
    "\n",
    "# 2. Fit & transform these features in merged_df, overwriting them with scaled values.\n",
    "combined_df[features_list] = scaler_med.fit_transform(combined_df[features_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1704fd-b842-407e-9537-5cc01af62c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Define mappings for common boolean-like values\n",
    "bool_mappings = {\n",
    "    'True': 1, 'False': 0,\n",
    "    'true': 1, 'false': 0,\n",
    "    True: 1, False: 0,\n",
    "    'Positive': 1, 'Negative': 0,\n",
    "    'Yes': 1, 'No': 0\n",
    "}\n",
    "\n",
    "# 4- Identify boolean or boolean-like columns\n",
    "bool_cols = []\n",
    "\n",
    "for col in combined_df.columns:\n",
    "    col_data = combined_df[col]\n",
    "    if col_data.dtype == 'bool':\n",
    "        bool_cols.append(col)\n",
    "    elif col_data.dtype == 'object' and set(col_data.dropna().unique()).issubset(set(bool_mappings.keys())):\n",
    "        bool_cols.append(col)\n",
    "    elif pd.api.types.is_numeric_dtype(col_data) and set(col_data.dropna().unique()).issubset({0, 1}):\n",
    "        bool_cols.append(col)\n",
    "\n",
    "# 5- Convert values in those columns using the mapping\n",
    "for col in bool_cols:\n",
    "    combined_df[col] = combined_df[col].map(bool_mappings).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705ec32-2b99-4eec-b52f-23d0d35a6805",
   "metadata": {},
   "source": [
    "## 3- Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bd880-8a10-4f48-8b38-c8da290e6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separate into X_med (feature matrix) and y_med (target vector)\n",
    "X_med = combined_df.drop(columns=[\"TARGET\"])\n",
    "y_med = combined_df[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd23f51-1eef-43be-88ad-8fdf0195a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12458f41-9be9-4387-998c-cbdcb6469a08",
   "metadata": {},
   "source": [
    "## 4- Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8e5c0-4998-446e-bc81-0ea6043c4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use StratifiedKFold to keep class imbalance roughly equal across folds.\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae938d-84c6-447a-a0c7-15b89034749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_med = {\n",
    "    'majority': {\n",
    "        'OCAN': {'y_true': [], 'y_pred': []},\n",
    "    },\n",
    "    'minority': {\n",
    "        'OCAN': {'y_true': [], 'y_pred': []},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff36fa-0f05-4d7b-b214-22a1b094579f",
   "metadata": {},
   "source": [
    "## 5- Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4f66-b12b-415b-a657-d8f8b310a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class OCAN:\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim=64,\n",
    "                 lr_g=1e-3,\n",
    "                 lr_d=1e-3,\n",
    "                 n_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 device='cpu'):\n",
    "        self.device = device\n",
    "        self.gen = Generator(input_dim, hidden_dim).to(device)\n",
    "        self.dis = Discriminator(hidden_dim // 2).to(device)\n",
    "        self.opt_g = optim.Adam(self.gen.parameters(), lr=lr_g)\n",
    "        self.opt_d = optim.Adam(self.dis.parameters(), lr=lr_d)\n",
    "        self.criterion_recon = nn.MSELoss()\n",
    "        self.criterion_adv   = nn.BCELoss()\n",
    "        self.n_epochs        = n_epochs\n",
    "        self.batch_size      = batch_size\n",
    "\n",
    "        # history dict for losses and metrics\n",
    "        self.history = {\n",
    "            'loss_d': [],   # discriminator loss\n",
    "            'loss_g': [],   # generator loss (recon + adv)\n",
    "            'auc':   [],    # validation AUC\n",
    "            'aupr':  []     # validation AUPR\n",
    "        }\n",
    "\n",
    "    def fit(self, X, X_val=None, y_val=None):\n",
    "        \"\"\"Train on X; if (X_val, y_val) provided, compute AUC/AUPR each epoch.\"\"\"\n",
    "        ds        = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "        loader    = DataLoader(ds, batch_size=self.batch_size, shuffle=True)\n",
    "        has_val   = (X_val is not None and y_val is not None)\n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_loss_d = 0.0\n",
    "            epoch_loss_g = 0.0\n",
    "            for (x_batch,) in loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "\n",
    "                # ——— Train Discriminator ———\n",
    "                with torch.no_grad():\n",
    "                    _, z_real = self.gen(x_batch)\n",
    "                z_fake    = torch.randn_like(z_real).to(self.device)\n",
    "                d_real    = self.dis(z_real)\n",
    "                d_fake    = self.dis(z_fake)\n",
    "                loss_d    = self.criterion_adv(d_real, torch.ones_like(d_real)) + \\\n",
    "                            self.criterion_adv(d_fake, torch.zeros_like(d_fake))\n",
    "                self.opt_d.zero_grad()\n",
    "                loss_d.backward()\n",
    "                self.opt_d.step()\n",
    "\n",
    "                # ——— Train Generator (autoencoder + adversarial) ———\n",
    "                recon, z  = self.gen(x_batch)\n",
    "                d_out     = self.dis(z)\n",
    "                loss_recon= self.criterion_recon(recon, x_batch)\n",
    "                loss_g_adv= self.criterion_adv(d_out, torch.zeros_like(d_out))\n",
    "                loss_g    = loss_recon + 1e-2 * loss_g_adv\n",
    "                self.opt_g.zero_grad()\n",
    "                loss_g.backward()\n",
    "                self.opt_g.step()\n",
    "\n",
    "                epoch_loss_d += loss_d.item()\n",
    "                epoch_loss_g += loss_g.item()\n",
    "\n",
    "            # average over batches\n",
    "            avg_d = epoch_loss_d / len(loader)\n",
    "            avg_g = epoch_loss_g / len(loader)\n",
    "            self.history['loss_d'].append(avg_d)\n",
    "            self.history['loss_g'].append(avg_g)\n",
    "\n",
    "            # if validation set provided, compute AUC/AUPR\n",
    "            if has_val:\n",
    "                raw_scores = self.score_samples(X_val)\n",
    "                # calibrate with your existing function\n",
    "                calib     = calibrate_scores(raw_scores, y_val, method=\"sigmoid\")\n",
    "                probs     = (calib.predict_proba(raw_scores.reshape(-1,1))[:,1]\n",
    "                             if hasattr(calib, \"predict_proba\")\n",
    "                             else calib.predict(raw_scores.reshape(-1,1)))\n",
    "                auc   = roc_auc_score(y_val, probs)\n",
    "                aupr  = average_precision_score(y_val, probs)\n",
    "                self.history['auc'].append(auc)\n",
    "                self.history['aupr'].append(aupr)\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs}  \"\n",
    "                      f\"loss_d={avg_d:.4f}  loss_g={avg_g:.4f}  \"\n",
    "                      f\"AUC={auc:.4f}  AUPR={aupr:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs}  \"\n",
    "                      f\"loss_d={avg_d:.4f}  loss_g={avg_g:.4f}\")\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        ds     = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for (x_batch,) in loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                recon, z = self.gen(x_batch)\n",
    "                rec_err  = torch.mean((recon - x_batch)**2, dim=1)\n",
    "                d_out    = self.dis(z).squeeze()\n",
    "                scores.append((rec_err + d_out).cpu())\n",
    "        return torch.cat(scores).numpy()\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Integrate OCAN into your cross-val loop\n",
    "# -----------------------------------------\n",
    "for r in range(repeats):\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_med, y_med), start=1):\n",
    "        X_train_m, X_test_m = X_med.values[train_idx], X_med.values[test_idx]\n",
    "        y_train_m, y_test_m = y_med.values[train_idx], y_med.values[test_idx]\n",
    "\n",
    "        # ---- Majority OCAN ----\n",
    "        X_train_major = X_train_m[y_train_m == 0]\n",
    "        ocan_major = OCAN(input_dim=X_train_major.shape[1],\n",
    "                          hidden_dim=64,\n",
    "                          n_epochs=30,\n",
    "                          batch_size=256,\n",
    "                          device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        ocan_major.fit(X_train_major)\n",
    "\n",
    "        raw_scores = ocan_major.score_samples(X_test_m)\n",
    "        # higher raw_scores → more anomalous\n",
    "        scores = raw_scores  # no need to negate\n",
    "        # calibrate\n",
    "        calib_sig = calibrate_scores(scores, y_test_m, method=\"sigmoid\")\n",
    "        prob_sig = (calib_sig.predict_proba(scores.reshape(-1,1))[:,1]\n",
    "                    if hasattr(calib_sig, \"predict_proba\")\n",
    "                    else calib_sig.predict(scores.reshape(-1,1)))\n",
    "        # evaluate\n",
    "        auc, aupr = roc_auc_score(y_test_m, prob_sig), average_precision_score(y_test_m, prob_sig)\n",
    "        results_med['majority']['OCAN'].append((auc, aupr))\n",
    "        predictions_storage['majority']['OCAN']['y_true'].extend(y_test_m.tolist())\n",
    "        predictions_storage['majority']['OCAN']['y_pred'].extend(prob_sig.tolist())\n",
    "        print(f\"Majority OCAN + Sigmoid: AUC={auc:.4f}, AUPR={aupr:.4f}\")\n",
    "\n",
    "        # ---- Minority OCAN ----\n",
    "        X_train_min = X_train_m[y_train_m == 1]\n",
    "        if len(X_train_min) > 0:\n",
    "            ocan_min = OCAN(input_dim=X_train_min.shape[1],\n",
    "                            hidden_dim=64,\n",
    "                            n_epochs=30,\n",
    "                            batch_size=256,\n",
    "                            device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            ocan_min.fit(X_train_min)\n",
    "\n",
    "            raw_scores_min = ocan_min.score_samples(X_test_m)\n",
    "            scores_min = raw_scores_min\n",
    "            calib_sig_min = calibrate_scores(scores_min, y_test_m, method=\"sigmoid\")\n",
    "            prob_sig_min = (calib_sig_min.predict_proba(scores_min.reshape(-1,1))[:,1]\n",
    "                            if hasattr(calib_sig_min, \"predict_proba\")\n",
    "                            else calib_sig_min.predict(scores_min.reshape(-1,1)))\n",
    "            auc_min = roc_auc_score(y_test_m, prob_sig_min)\n",
    "            aupr_min = average_precision_score(y_test_m, prob_sig_min)\n",
    "            results_med['minority']['OCAN'].append((auc_min, aupr_min))\n",
    "            predictions_storage['minority']['OCAN']['y_true'].extend(y_test_m.tolist())\n",
    "            predictions_storage['minority']['OCAN']['y_pred'].extend(prob_sig_min.tolist())\n",
    "            print(f\"Minority OCAN + Sigmoid: AUC={auc_min:.4f}, AUPR={aupr_min:.4f}\")\n",
    "\n",
    "    print(f\"--- Finished Medicare Repeat {r+1} with OCAN ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2c74e-6f0f-4efe-b2de-17a021acc32d",
   "metadata": {},
   "source": [
    "## 6- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a240c4-def5-40d3-b90c-d84b8e06fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in results_med:\n",
    "    print(f'\\n>>> {group.upper()}')\n",
    "    for method in results_med[group]:\n",
    "        scores = results_med[group][method]\n",
    "        if scores:\n",
    "            mean_auc, mean_aupr = compute_mean_scores(scores)\n",
    "            print(f'{method:20s} | AUC: {mean_auc:.4f} | AUPRC: {mean_aupr:.4f}')\n",
    "        else:\n",
    "            print(f'{method:20s} | No scores available.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
